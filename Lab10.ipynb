{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidad del Valle de Guatemala <br>\n",
    "Facultad de Ingeniería <br>\n",
    "Departamento de Ciencias de la Computación <br>\n",
    "Deep Learning <br>\n",
    "\n",
    "# Laboratorio 10\n",
    "\n",
    "Integrantes\n",
    "- Gabriel Vicente\n",
    "- Christopher García\n",
    "- Marco Orozco\n",
    "- Rodrigo Barrera\n",
    "- Isabel Solano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrucciones:\n",
    "- Deben estar unido a uno de los grupos de Canvas de nombre “Laboratorio 9,10 # - N”, donde N es un número entre 1 y 15. Los grupos pueden ser de máximo 5 personas.\n",
    "- Esta actividad debe realizarse en grupos.\n",
    "- Sólo es necesario que una persona del grupo suba el trabajo a Canvas.\n",
    "- No se permitirá ni se aceptará cualquier indicio de copia. De presentarse, se procederá según el reglamento correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Práctica\n",
    "Para esta parte estarán resolviendo el problema de CartPole con Deep Q-Learning y una red de destino. Para esto, el objetivo de este ejercicio es entrenar a un agente para que equilibre un poste en un carro (cartpole) en movimiento\n",
    "durante el mayor tiempo posible. Se deberá usar Deep Q-Learning (DQL) con una red objetivo para lograr esto. Para realizar este ejercicio necesitará:\n",
    "1. Python con las bibliotecas necesarias, incluidas Gymnasium, NumPy y PyTorch (para este caso, pueden usar otro framework de Deep Learning si no se sienten cómodos con PyTorch).\n",
    "2. El entorno CartPole proporcionado por Gymnasium.\n",
    "\n",
    "\n",
    "Considere las siguientes instrucciones generales para realizar este ejercicio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Liberías\n",
    "Asegúrese de tener instalado Gymnasium, NumPy y el framework de Deep Learning que haya\n",
    "elegido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cree el entorno CartPole\n",
    "Utilice la biblioteca Gymnasium para crear el entorno CartPole. Este entorno simula la tarea de equilibrar un poste en un carro en movimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defina las redes en línea y de destino\n",
    "Cree dos redes neuronales, la red en línea y la red de destino. La red en línea se utiliza para la selección de acciones y se actualiza con más frecuencia, mientras que la red de destino se utiliza para estimar los valores Q y se actualiza periódicamente. Ambas redes deberían tener una arquitectura similar con capas de entrada y salida. Inicialmente, la red de destino debería tener los mismos pesos que la red en línea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Establecer hiperparámetros\n",
    "Defina hiperparámetros como el número de episodios, el tamaño de los batches, el factor de descuento (gamma) y los parámetros de exploración (epsilon, epsilon decay, epsilon mínimo). Ajuste estos hiperparámetros según sea necesario para optimizar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Defina la selección de acciones épsilon-greedy\n",
    "Cree una función para la selección de acciones\n",
    "épsilon-greedy. Esta función ayuda al agente a elegir acciones basadas en la política épsilon-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Defina la reproducción de la experiencia (experience replay)\n",
    "Implemente una función para la\n",
    "reproducción de la experiencia, que es una parte crucial de DQL. Esta función ayuda al agente a aprender\n",
    "de una memoria de repetición y a estabilizar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ciclo de entrenamiento\n",
    "Cree un ciclo para el entrenamiento del agente. En cada episodio, el agente\n",
    "interactúa con el entorno, recopila experiencias y actualiza sus valores Q mediante la repetición de\n",
    "experiencias (experience replay). La red de destino se actualiza cada N episodios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Representar el entorno\n",
    "Para visualizar el progreso del entrenamiento del agente, use env.render() para\n",
    "mostrar el entorno CartPole durante el entrenamiento. Asegúrese de llamar a env.close() al final para\n",
    "limpiar el renderizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Supervisar el entrenamiento\n",
    "Supervise el progreso del entrenamiento del agente, incluida la recompensa\n",
    "total obtenida en cada episodio, para esto utilice una gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Evalúe el rendimiento\n",
    "Una vez que se complete el entrenamiento, evalúe el rendimiento del agente\n",
    "probándolo en el entorno CartPole sin renderizar y observe qué tan bien puede equilibrar el poste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Fine-Tuning\n",
    "Experimente con diferentes hiperparámetros, arquitecturas y estrategias de entrenamiento\n",
    "para mejorar el desempeño del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Notas adicionales\n",
    "Tenga en cuenta que los tiempos de entrenamiento pueden variar y puede ajustar la\n",
    "frecuencia de actualización de la red de destino según los requisitos específicos de su tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Teoría\n",
    "Defina en qué consiste y en qué clase de problemas se pueden usar cada uno de los siguientes acercamientos en\n",
    "Deep Reinforcement Learning\n",
    "\n",
    "1. Proximal Policy Optimization\n",
    "\n",
    "RES\n",
    "\n",
    "2. Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "RES\n",
    "\n",
    "3. Trust Region Policy Optimization (TRPO)\n",
    "\n",
    "RES\n",
    "\n",
    "4. Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "RES"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
