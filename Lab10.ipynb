{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidad del Valle de Guatemala <br>\n",
    "Facultad de Ingeniería <br>\n",
    "Departamento de Ciencias de la Computación <br>\n",
    "Deep Learning <br>\n",
    "\n",
    "# Laboratorio 10\n",
    "\n",
    "Integrantes\n",
    "- Gabriel Vicente\n",
    "- Christopher García\n",
    "- Marco Orozco\n",
    "- Rodrigo Barrera\n",
    "- Isabel Solano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrucciones:\n",
    "- Deben estar unido a uno de los grupos de Canvas de nombre “Laboratorio 9,10 # - N”, donde N es un número entre 1 y 15. Los grupos pueden ser de máximo 5 personas.\n",
    "- Esta actividad debe realizarse en grupos.\n",
    "- Sólo es necesario que una persona del grupo suba el trabajo a Canvas.\n",
    "- No se permitirá ni se aceptará cualquier indicio de copia. De presentarse, se procederá según el reglamento correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Práctica\n",
    "Para esta parte estarán resolviendo el problema de CartPole con Deep Q-Learning y una red de destino. Para esto, el objetivo de este ejercicio es entrenar a un agente para que equilibre un poste en un carro (cartpole) en movimiento\n",
    "durante el mayor tiempo posible. Se deberá usar Deep Q-Learning (DQL) con una red objetivo para lograr esto. Para realizar este ejercicio necesitará:\n",
    "1. Python con las bibliotecas necesarias, incluidas Gymnasium, NumPy y PyTorch (para este caso, pueden usar otro framework de Deep Learning si no se sienten cómodos con PyTorch).\n",
    "2. El entorno CartPole proporcionado por Gymnasium.\n",
    "\n",
    "\n",
    "Considere las siguientes instrucciones generales para realizar este ejercicio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Liberías\n",
    "Asegúrese de tener instalado Gymnasium, NumPy y el framework de Deep Learning que haya\n",
    "elegido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cree el entorno CartPole\n",
    "Utilice la biblioteca Gymnasium para crear el entorno CartPole. Este entorno simula la tarea de equilibrar un poste en un carro en movimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defina las redes en línea y de destino\n",
    "Cree dos redes neuronales, la red en línea y la red de destino. La red en línea se utiliza para la selección de acciones y se actualiza con más frecuencia, mientras que la red de destino se utiliza para estimar los valores Q y se actualiza periódicamente. Ambas redes deberían tener una arquitectura similar con capas de entrada y salida. Inicialmente, la red de destino debería tener los mismos pesos que la red en línea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Establecer hiperparámetros\n",
    "Defina hiperparámetros como el número de episodios, el tamaño de los batches, el factor de descuento (gamma) y los parámetros de exploración (epsilon, epsilon decay, epsilon mínimo). Ajuste estos hiperparámetros según sea necesario para optimizar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Defina la selección de acciones épsilon-greedy\n",
    "Cree una función para la selección de acciones\n",
    "épsilon-greedy. Esta función ayuda al agente a elegir acciones basadas en la política épsilon-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Defina la reproducción de la experiencia (experience replay)\n",
    "Implemente una función para la\n",
    "reproducción de la experiencia, que es una parte crucial de DQL. Esta función ayuda al agente a aprender\n",
    "de una memoria de repetición y a estabilizar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ciclo de entrenamiento\n",
    "Cree un ciclo para el entrenamiento del agente. En cada episodio, el agente\n",
    "interactúa con el entorno, recopila experiencias y actualiza sus valores Q mediante la repetición de\n",
    "experiencias (experience replay). La red de destino se actualiza cada N episodios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Representar el entorno\n",
    "Para visualizar el progreso del entrenamiento del agente, use env.render() para\n",
    "mostrar el entorno CartPole durante el entrenamiento. Asegúrese de llamar a env.close() al final para\n",
    "limpiar el renderizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Supervisar el entrenamiento\n",
    "Supervise el progreso del entrenamiento del agente, incluida la recompensa\n",
    "total obtenida en cada episodio, para esto utilice una gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Evalúe el rendimiento\n",
    "Una vez que se complete el entrenamiento, evalúe el rendimiento del agente\n",
    "probándolo en el entorno CartPole sin renderizar y observe qué tan bien puede equilibrar el poste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Fine-Tuning\n",
    "Experimente con diferentes hiperparámetros, arquitecturas y estrategias de entrenamiento\n",
    "para mejorar el desempeño del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Notas adicionales\n",
    "Tenga en cuenta que los tiempos de entrenamiento pueden variar y puede ajustar la\n",
    "frecuencia de actualización de la red de destino según los requisitos específicos de su tarea."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAABfCAYAAADh2BDxAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAA5rSURBVHhe7Z09jhQ7FEZ7npBYBgSsAyRgDYQQICE2AUKwDQhgAaQkEMA6JgCRs4KReHWa/pCx7Cq72lVd0/6OZE23/319r6/tqoaLq6ur3ztjjDHd8d/hrzHGmM6wAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE6xAzDGmE45awfw/v37w6d6vn//vnv69Onh2/FQF3Ua0wuvX78+fKqntf2dAtafY2SwBhdXV1e/D5/PChTozp07u2F8h5hyVPbLly+7e/fuHWKPA0X4+vXr7t27d7vbt28fYo3ZJtgA+vrjx4/drVu39jpbaws3btyYbX8s/i9fvmxmf6dCDoCxbJGzOgGgOBI4ChsutMSTXgLK13LxBynzhw8fDjHGbAvZD5sf9BT7efz48f4vzoB42VcKyoen7tD+iK+xP2zlui/+gN0jS+S3Rc7KAbBbQdhSUhQIpXvw4MFfhZ6CsuRbQvmkDGNGZMwp0MKPDV1eXv7dsMgW+P758+e9/pIvt5iTjr2RHtrfmzdv9nVPIdugvXMBuW31CvjsroAQMt7227dv+88o3YsXL3ZPnjw55MhDfpR7zrG1FPqGQWBkJQ7JmKVBH9FLrien7EQ2gu6iwynY7WN/1Mn1EaeIGvs7R9vAASCLrTm2s3sIHO7eWfy1iynh4cOHi0+Q+sNOyZhTo8WfK8+SRRrdJR+LNYtaCtmfnh2U2h+nhJr81wmcIKcb5LYlzu4ZAAqNYrKbQRHZ/fN96ghGGoGJWpqtKoPpCy3+uu4pRTZC2RD0GTtjI4XdAX91vTRlf5wcVO7c0MYPJ7clzu4EgKDZzQAKitC5g2M3MnYHqWcEa+w+MDjaiQ3ImLVgUUb/sJfaUy+2BCzasQ6TFl7h8JfNGE6jxP5KTiHXFZxbzcPwNTjb10BB94klkBclXeuOjvYwFjkrY9aCRZvdP8x9241XPGHsuUHNa6Bhf84Vyb3kWctaXIsTgI6HNTtmyminMgV5j/HKlKVvNX3E6Mh7TLtbgTGwoySUyEDylswEcSU7JMophHkpyxVEHB9DmoLqEYobGwd5SsYZojKlMpoL7UxBHwAdnLP4h4zt6ksXOcl8bl9K5qwlmssSWYcwPk45PCDfCpt2AEwmHpM7RX1HeRG8DD0Hgi7dTeiBbO3un37Qv/CBLp9lYGPcvXt3/3fMgK4DjJUxo9y6GyYuJwPmjPnkBITsdCeqepAH6RhYDHHsKpE7gfyUJx59YM45ZlPvmG5QLuwD30F9ENRJHhm69JE8tMV30nNjFaqXMiUyUruMNQxhfskiDrQxBn2WbNSXYxjbZLHTLUH9qXUAof1pjviM/JYAmYf6wl/ipBclMEbKbAaugLYYhsWbq6n93zB+WKR/D0q+TxsU7J+0uUF1ptJyIdc/wrDzScaH4fLysukYThGQW6r/GtuYTMM81KF6qJN4ZBjmV3zcnuKpL4wL8+QCdaktysRzpj4ORrtPi/tEII08ufmm3rkyok7pempMGjt9CMc/FhgDZQip9JJAv1RHamy1QeNIpeWC5JeSO/Wl5HVMkNxCOfOZeM1RyRxorKXztXTYpAOQgqHYcZomvqUQaWfMEFOBMqkFQX2fMgyNI1VHSUCR6HOrkDKkqTBWVgvjmBxUPpQBcuF7WKdkRV7FhWEsbSxoriibWzDUx5QuEmTQU+VTMmKcUzIiTXXE+k6btfNGPbQ5V+8I9Ik6CC1sUDJMpeUC/U+NQXN6zPjiIF1OyVrtlfZfsqudt6XCJq+AdIQbhLT/GzIo8OHTv5/XhmNf6vjLUZR+DUpziEmjvuvoWgvH90GZmoWp/uag/6kxqL6pKy7K6joMkEvcH9WRu26gTK4fJVAudx1Cm6Tn5KM+5cap8qm+qezYnfCwkO37RnldPcCcaxPKqB85WZYQ9reFDSK72nq4RknJFKirxfUWIDMC85CStfpAeg25vq/N5hwA95wIJydQ3Z/VCnwMFLDGIDR5GGR8j0u/hl1RkUIfYzyURSFbhTnwei2LdWouphbGkKn2S42lpK0cU3Mxd8GUgx2T0RTDDnkvIz03QB78Jb6GUI7HOIDWNlg6v0L5me/Y/pAT9jdXp2NUf86hyBmGm5jrxCYdAOR+ELKEwFGomsVYi68MkYdvJQ8Dzw3koEUAWbBTQgaEcLd6LFMLjeZvrtGXlKvRj5BWMuI0TF2U44ScOh3XMFdW4QPMVj/aqt2AIQecX2x/yIW4VjBW5gty8ppKj5mrR0uxKQcQKldOULUCL4G2ahWHXV24A5MyooiltFTWU8EYMDzeqmFumBfkUnMELzEK6qX+WGbSh1ZH/iWQbhwjI0DngAVzju60WHzktHBqLRez2tMbsovtj/UDGc+RTQptNsc2IGqrpSzWZJPPAK6DwGkfBeS4yW5Mykj/al5DmzsO2mkZ5oDByeEhB2TQ0jGHsPghKy2i9Jn2ef2OdsPFYEtIRvT5WBnJeTD2OSescJc957qMscjhtvwnG+ZcR4X2R5B+IJsa+xtDY83dNiAPmDqhhsjW5tp9azblAORxcwqREvjcxSuE9koNIm6PidSODkWEkj4pzxzlB95/Z2FpFWrliHFgaIxdO9MlYbwydNrW7wdC57s1QhmVXtnk5oG60FHqoT5OFIQaShadMT0IZd5yAdPCXQL5wryUJbAm8EwK5ji3MZB3itR1tJxGDvVtrt23ZlMOQELJCSclcCnlMZQqIJOrBTOFlJGwNCyEV1dXzUJtn7UDrbnGOGZnhvNnjuRsGT9GnzPOLdBKRoybeDlaLcA4gBK9DdHmKT5BSLeJp954IeOkRVtyQEtQMhb6yCkwBTKhb2vphGQUtld6MltjjShhkw4g58F1Amg9wWp3SgE1uWPtU0eJwWuMaylrazQXOUWWs24F7UztrrbGlD5JRmG+lDxZ/LW7FXwnb+2dt65uQllSnkVfV1SEcCFj8Sf/Uot/6QsdJfaPXeXsjzEgy1J5ja1H1KV6NGf0r2QsOZuBX79+7Z4/f777+PHjIWZZNuUANLEpQ2fiYsGVCnyKsXZDaItdmHZiMRhK6a5UbS1hUGugnWTqGkJjY75CY4tPdkorMUjkhA4gY/6GQTvWUsMWyp/bcMBYWkiqbc1taldYMv/UyXgh1n2+U5Y8yKB07JRDf1UO6EtKp4mnfWSw5M5fuiSZ5KDv5M3ZH+NBx1L9lCxZMzTuKXJXi9QlvQ/bwqFPyYhysR2EPHv2bPf27dvdo0ePdj9//jzELshw/E/+QuxUYZjc/a8VB+Hvvw+Kt/8l3rA7+furO9L4Tnxcfm6gzUG5kmkKtEn/1L7i1Ef1uSSQv2X/1w6MG3kxH/xFBsQhA40LWWm+iCNd5ZA3aQTJfkr+yj8WaDNVNgzkifug9ukfefhMn5WuPqp+1aH0sA61k5MRZXMyIo8+h3XHuqVyCvSPMurfVKAfKqO+hWnEkx63u1SQfFNpYWB89En9pa98pr9TfaUNyYtyqTxxoE7Kae74rn4qTX2Yap98tD02R69evdrnubi4SKa3Dpv856DxsOwGtPsaBLv/C6Rx748XDeOPRbuoYSIPMXnIx66O/mnHQaiBd5fpf8sxnALNFbuflCyUPhjN3++5XTVpyheinehgOPu6B6Pbx5MfqI88zAlx5EvVI8ibQu2P9RHoAztJ9SMkNQbiQhkRwjxKV1yuf6Fcc3kgzDdG2K8Y4iXvGMY+Jt85sDMmDOvRISZPLE/6WDtm5q60DONVW1wvhfMepk3ZMnn1MsMYnz592t2/f3938+bNQ8xynPX/B1ADSsEiw+SkDLsltIXDmVIE88dgefBX6izJizGWOHKTB1tggYxljt5yFdraAaxpfziaU2y8dEV1irZzbPJ3AKcAZUfxUve1rUERat4M6RnNR6m8mEcWE3McOlWF8H2J3T/I/lq81TfF2OluKdjIILvSU8da2AEE8IYEuwMmaymom2AHUIYMptRoke3SO8geYKeqkyo2wcmKHfqSu1eunGhzaftjbGvDRgZd3poD8BVQRO7o24ql6z9HkBkOQK8+5mCHxYLF9c/WDO26gczjEwCyX/raEmcDS9rf2teDOB0c6BrXW7XYAUQwWbxbPbXYzAHlxqh8P10PCztzw8Ku05Pmh4VfV0UsHF78j0eLllhj8Re0m3sAfQzoCRuJtTdfW9702QEkQPlZcFDCVk5AjsUPfo8DB0oIr4R4KImBbW13dd1BZ5E1cl3TqcpWWm/CqHdtHWHTh66ylmwRO4AMKD67yhYT58XfmDqwmdabsLXRZmXL1712AMYY0yl2AMYsBLvY+DVWX1OZLWEHYMwChA/8cQS83853HMGWrwRMX/h3AMY0hrtrdv5624tdPws/juAU76Abk8MOwJiGsMvndcP4f8xi8Qe/omq2hK+AjGmIfkA12NUh5g/6HYN/A2K2hE8AxjSExT/1b+UQ7wfAZmvYARjTCF3zpO75SdN/XqR8xpwaOwBjGhM7AJ4JgO7/9WqoMafGDsCYRnDFwyIf/jMVXP1owScdZ+AHwWYr2AEY0xD+6QK9CaTfAhDHos9nnIMdgNkKfgvIGGM6xScAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpFDsAY4zpkt3uf8diJodrc8NqAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Teoría\n",
    "Defina en qué consiste y en qué clase de problemas se pueden usar cada uno de los siguientes acercamientos en\n",
    "Deep Reinforcement Learning\n",
    "\n",
    "1. Proximal Policy Optimization (PPO)\n",
    "\n",
    "Proximal Policy Optimization (PPO) es un algoritmo introducido por OpenAi en 2017 con el fin de implementar el aprendizaje por refuerzo. Estos algoritmos son policy gradient methods (métodos de gradiente de políticas) lo que significa que en vez de asignarle valor a una pareja de estado-acción, estas buscan en un espacio de políticas. Tienen los beneficios de un TRPO, pero son más simples de implementar, más generales y tienen una mejor complejidad de prueba. \n",
    "\n",
    "2. Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "Es un algoritmo que de forma confurrente aprende una Q-function y una política. Utiliza datos de off-policy y la ecuación de Bellman para aprender la Q-function y usa esa función para aprender la política.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "3. Trust Region Policy Optimization (TRPO)\n",
    "\n",
    "Similar a policy gradient methods, este algoritmo es escalable y efectivo para optimizar políticas grandes (con miles de parámetros) no lineales como una red neuronal. Este algoritmo puede ser utilizado en ambientes con espacios de acción tanto discretos como continuos. Es posible paralelizar con MPI. TRPO tiene 2 variantes, la primera, *single-path* method puede ser aplicada en un set model-free, mietnras que la segunda, *vine* requiere que el sistema sea recuperado en ciertos estados particulares solamente posible en una simulación.\n",
    "\n",
    "4. Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "Es un algorimto de aprendizaje reforzado que permite a los agentes aprender las acciones más óptimas en ambientes complejos. Este algoritmo fue desarrollado por Google's DeepMind. Este algoritmo tiene este nombre porque, primero, y a diferencia de otros algorimtos populares de aprendizaje reforzado, este algorimto utiliza múltiples agentes en un mismo ambiente. Cada agente tiene su propia red de parámetros y copia del ambiente, y cada uno interactua con su ambiente de manera asíncrona, aprendiendo con cada interacción controlada por una red global. También se le llama Arctor-critic porque cada agente de aprendizaje usa el valor de la función de valor para actualizar la función de política óptima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "- John Schulman et al. (s.f.), Trust Region Policy Optimization, recuperado de: https://arxiv.org/pdf/1502.05477.pdf\n",
    "- Wikipedia (2023), Proximal Policy Optimization, recuperado de: https://en.wikipedia.org/wiki/Proximal_Policy_Optimization#References\n",
    "- OpenAI (s.f.), \"Trust Region Policy Optimization\", recuperado de: https://spinningup.openai.com/en/latest/algorithms/trpo.html\n",
    "- OpenAI (s.f.), \"Deep Deterministic Policy Gradient\", recuperado de: https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "- AlindGupta (2023), \"Asynchronous advantage actor critic, recuperado de Geeksforgeeks: https://www.geeksforgeeks.org/asynchronous-advantage-actor-critic-a3c-algorithm/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
